# ###########################################
# Setup Configuration
# ###########################################
env_cfg:
  run_name: test
  project_name: fmir
  task: bsr
  out_dir: /data/out
  seed: 2025


# -------------------------------------------------
# Dataset
# -------------------------------------------------
dataset_cfg:
  train_dataset: { name: Imagenet, path: /local_datasets/Imagenet, task: bsr, batch_size: 32, patch_size: 256, num_workers: 16}
  val_dataset: { name: Imagenet256, path: /local_datasets/Imagenet256, task: bsr, batch_size: 16, patch_size: 256, num_workers: 16}


# -------------------------------------------------
# Flow-Matching
# -------------------------------------------------
fm_cfg:
  method: l2_cfm_mse_loss
  k_steps: 3
  t_emb_dim: 160
  sigma_min: 0.00001
  sigma_s: 0.04
  dt: 0.2
  alpha: 0.001
  beta: 0.001


# -------------------------------------------------
# Model
# -------------------------------------------------
model_cfg:
  arch_cfg: {name: elir,
             params: { fm_cfg: {k_steps: 3, sigma_s: 0.04, latent_shape: [16,32,32]},
                       fmir_cfg: { name: lunet,
                                   params: { ch_mult: [1,2,1,2], n_mid_blocks: 1, in_channels: 16, hid_channels: 96, out_channels: 16,
                                             t_emb_dim: 160, overparametrization: True },
                                   trainable: True },
                       mmse_cfg: { name: rrdbnet,
                                   params: { c_inout: 16, c_hid: 96, n_rrdb: 3, overparametrization: True },
                                   trainable: True },
                       enc_cfg: {name: tiny_enc,
                                trainable: True},
                       dec_cfg: {name: tiny_dec,
                                trainable: False}}}

  teacher_cfg: {name: taesd,
                params: { },
                trainable: False}
# -------------------------------------------------
# Training
# -------------------------------------------------
train_cfg:
  epochs: 50
  optimizer: !name:torch.optim.AdamW
  lr: 0.0002
  optimizer_params: {}
  ema_decay: 0.999
  max_steps: -1 # run only several steps
  check_val_every_n_epoch: 1
  num_sanity_val_steps: 2
  wandb: False



# -------------------------------------------------
# Evaluation
# -------------------------------------------------
eval_cfg:
  metrics: ["lpips","psnr","niqe","musiq","clipiqa"]
